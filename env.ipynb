{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.4.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.16.1\n",
      "\n",
      "Python 3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:35:41) \n",
      "[Clang 16.0.6 ]\n",
      "Pandas 2.2.1\n",
      "Scikit-Learn 1.4.2\n",
      "SciPy 1.13.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "# print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-xiangqi in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: gym in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym-xiangqi) (0.26.0)\n",
      "Requirement already satisfied: wheel in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym-xiangqi) (0.41.2)\n",
      "Requirement already satisfied: pygame in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym-xiangqi) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym->gym-xiangqi) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym->gym-xiangqi) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym->gym-xiangqi) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from gym->gym-xiangqi) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/miniconda3/envs/py39/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym->gym-xiangqi) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-xiangqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "from gym_xiangqi.constants import RED, BLACK, PIECE_ID_TO_NAME, ALLY\n",
    "from gym_xiangqi.utils import action_space_to_move\n",
    "from gym_xiangqi.agents import RandomAgent\n",
    "\n",
    "\n",
    "import random\n",
    "import time \n",
    "import numpy as np\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import load_model, save_model, Sequential\n",
    "# from tensorflow.keras.optimizers.legacy import RMSprop      #Runs faster for M1/M2 macs. Use the line below for any other system\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (10, 9)\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -9,  -7,  -5,  -3,  -1,  -2,  -4,  -6,  -8],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, -11,   0,   0,   0,   0,   0, -10,   0],\n",
       "       [-16,   0, -15,   0, -14,   0, -13,   0, -12],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 12,   0,  13,   0,  14,   0,  15,   0,  16],\n",
       "       [  0,  10,   0,   0,   0,   0,   0,  11,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  8,   6,   4,   2,   1,   3,   5,   7,   9]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('gym_xiangqi:xiangqi-v0')\n",
    "env.reset()\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     time.sleep(1)\n",
    "#     # env.render()\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 129600 possible actions and 10 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  \n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(129600)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9  -7  -5  -3  -1  -2  -4  -6  -8]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0 -11   0   0   0   0   0 -10   0]\n",
      " [-16   0 -15   0 -14   0 -13   0 -12]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [ 12   0  13   0  14   0  15   0  16]\n",
      " [  0  10   0   0   0   0   0  11   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  8   6   4   2   1   3   5   7   9]]\n",
      "129600\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "observation= env.reset()\n",
    "print(observation)\n",
    "q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84776\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1\n",
      "RL Agent made the move SOLDIER_2 from [7, 3] to [1, 6].\n",
      "Reward: -10.0\n",
      "================\n",
      "Round: 2\n",
      "RL Agent made the move ELEPHANT_2 from [9, 3] to [9, 2].\n",
      "Reward: -10.0\n",
      "================\n",
      "Round: 3\n",
      "RL Agent made the move CANNON_1 from [4, 1] to [2, 7].\n",
      "Reward: -10.0\n",
      "================\n",
      "Round: 4\n",
      "RL Agent made the move SOLDIER_5 from [7, 0] to [3, 6].\n",
      "Reward: -10.0\n",
      "================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mround\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# if env.turn == ALLY:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#     obs, reward, done, info = env.step_user()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#     piece = PIECE_ID_TO_NAME[piece]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     22\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_xiangqi:xiangqi-v0')\n",
    "env.reset()\n",
    "# env.render()\n",
    "agent = RandomAgent()\n",
    "\n",
    "done = False\n",
    "round = 0\n",
    "    \n",
    "while not done:\n",
    "    # if env.turn == ALLY:\n",
    "    #     obs, reward, done, info = env.step_user()\n",
    "\n",
    "    #     if \"exit\" in info and info[\"exit\"]:\n",
    "    #         break\n",
    "\n",
    "    #     player = \"You\"\n",
    "    #     piece, start, end = env.user_move_info\n",
    "    #     piece = PIECE_ID_TO_NAME[piece]\n",
    "    # else:\n",
    "    time.sleep(1)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    player = \"RL Agent\"\n",
    "    move = action_space_to_move(action)\n",
    "    piece = PIECE_ID_TO_NAME[move[0]]\n",
    "    start = move[1]\n",
    "    end = move[2]\n",
    "\n",
    "    # env.render()\n",
    "    round += 1\n",
    "    print(f\"Round: {round}\")\n",
    "    print(f\"{player} made the move {piece} from {start} to {end}.\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(\"================\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from llamagym import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XiangQiAgent(RandomAgent):\n",
    "    def get_system_prompt(self) -> str:\n",
    "        return \"\"\"\"You are a skilled Xiangqi player. Each turn, you'll observe the current board state, including the positions of your pieces and the opponent's pieces, as well as the status of your general and the opponent's general. The game objective is to checkmate the opponent's general while safeguarding your own.\n",
    "                Decide your next move by providing an action code:\n",
    "                \"Action: 0\" to maintain your current position and strategy.\n",
    "                \"Action: 1\" to make a move or capture an opponent's piece.\n",
    "                Prioritize capturing opponent pieces when strategically advantageous. Protect your general and establish control over key points on the board. Utilize tactics such as pinning, forking, and skewering to gain an advantage.\n",
    "\n",
    "                End the game by checkmating the opponent's general or securing a favorable position. Avoid reckless moves that expose your general to immediate threats.\n",
    "                Adapt your strategy based on the opponent's moves and board developments. Continuously assess the board and adjust your tactics accordingly.\n",
    "                By following these guidelines, aim to outmaneuver your opponent and achieve victory in the game of Xiangqi.\"\"\"\n",
    "\n",
    "    def format_observation(self, observation: gym.core.ObsType) -> str:\n",
    "        return f\"You: {observation[0]}. Dealer: {observation[1]}. You have {'an' if bool(observation[2]) else 'no'} ace.\"\n",
    "\n",
    "    def extract_action(self, response: str) -> gym.core.ActType:\n",
    "        match = re.compile(r\"Action: (\\d)\").search(response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "        digits = [char for char in response if char.isdigit()]\n",
    "        if len(digits) == 0 or digits[-1] not in (\"0\", \"1\"):\n",
    "            if \"stick\" in response.lower():\n",
    "                return 0\n",
    "            elif \"hit\" in response.lower():\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"env\": \"gym_xiangqi:xiangqi-v0\",\n",
    "    \"lora/r\": 16,\n",
    "    \"lora/lora_alpha\": 32,\n",
    "    \"lora/lora_dropout\": 0.05,\n",
    "    \"lora/bias\": \"none\",\n",
    "    \"lora/task_type\": \"CAUSAL_LM\",\n",
    "    # \"load_in_8bit\": True,\n",
    "    \"batch_size\": 8,\n",
    "    \"seed\": 42069,\n",
    "    \"episodes\": 5000,\n",
    "    \"generate/max_new_tokens\": 32,\n",
    "    \"generate/do_sample\": True,\n",
    "    \"generate/top_p\": 0.6,\n",
    "    \"generate/top_k\": 0,\n",
    "    \"generate/temperature\": 0.9,\n",
    "}\n",
    "\n",
    "device = 'cpu'\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    **{\n",
    "        key.split(\"/\")[-1]: value\n",
    "        for key, value in hyperparams.items()\n",
    "        if key.startswith(\"lora/\")\n",
    "    }\n",
    ")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=hyperparams[\"model_name\"],\n",
    "    peft_config=lora_config,\n",
    "    # load_in_8bit=hyperparams[\"load_in_8bit\"],\n",
    "    token=HF_TOKEN,\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams[\"model_name\"], token=HF_TOKEN)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "model.pretrained_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "agent = XiangQiAgent(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    {\n",
    "        key: value\n",
    "        for key, value in hyperparams.items()\n",
    "        if key.startswith(\"generate/\")\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": hyperparams[\"batch_size\"],\n",
    "        \"mini_batch_size\": hyperparams[\"batch_size\"],\n",
    "    },\n",
    ")\n",
    "env = gym.make(hyperparams[\"env\"], natural=False, sab=False)\n",
    "\n",
    "for episode in trange(hyperparams[\"episodes\"]):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(observation)\n",
    "        print(action)\n",
    "        # wandb.log({\"action\": action})\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.assign_reward(reward)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    episode_stats = {\n",
    "        \"episode\": episode,\n",
    "        \"total_return\": sum(agent.current_episode_rewards),\n",
    "        \"message_ct\": len(agent.current_episode_messages),\n",
    "        \"episode_messages\": agent.current_episode_messages,\n",
    "    }\n",
    "    train_stats = agent.terminate_episode()\n",
    "    episode_stats.update(train_stats)\n",
    "    # wandb.log(episode_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseChessAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):        \n",
    "        \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            # NOTE: fill this in\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = ChineseChessAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = nn.Linear(input_size, 16)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(16, 16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        # Output layer without activation function\n",
    "        self.output_layer = nn.Linear(16, output_size)\n",
    "\n",
    "        # Initialization using Xavier uniform (a popular technique for initializing weights in NNs)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class QNetwork:\n",
    "    def __init__(self, env, lr, logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.net = FullyConnectedModel(9, 2)\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
    "        # Initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.memory = collections.deque([], maxlen=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        # Returns a batch of randomly sampled transitions to be used for training the model.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        # Appends a transition to the replay memory.\n",
    "        self.memory.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"gym_xiangqi:xiangqi-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, environment_name, lr=5e-4, render=False):\n",
    "        # Initialize the DQN Agent.\n",
    "        self.env = gym.make(environment_name)\n",
    "        self.lr = lr\n",
    "        self.policy_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # Copy the weight of the policy network\n",
    "        self.rm = ReplayMemory(self.env)\n",
    "        self.burn_in_memory()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.c = 0\n",
    "\n",
    "    def burn_in_memory(self):\n",
    "        # Initialize replay memory with a burn-in number of episodes/transitions.\n",
    "        cnt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Iterate until we store \"burn_in\" buffer\n",
    "        while cnt < self.rm.burn_in:\n",
    "            # Reset environment if terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                state = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            # Randomly select an action (left or right) and take a step\n",
    "            action = torch.tensor(random.sample([0, 1], 1)[0]).reshape(1, 1)\n",
    "            obs, reward, done, info = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "            # Store new experience into memory\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "            state = next_state\n",
    "            cnt += 1\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
    "        p = random.random()\n",
    "        if p > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.greedy_policy(q_values)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "\n",
    "    def greedy_policy(self, q_values):\n",
    "        return torch.argmax(q_values)\n",
    "        \n",
    "    def train(self):\n",
    "        # Train the Q-network using Deep Q-learning.\n",
    "        state = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "\n",
    "        # Loop until reaching the termination state\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "\n",
    "            # Decide the next action with epsilon greedy strategy\n",
    "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            obs, reward, done, info = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Store the new experience\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Sample minibatch with size N from memory\n",
    "            transitions = self.rm.sample_batch(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Get current and next state values\n",
    "            action_batch = action_batch.unsqueeze(1)\n",
    "            print(action_batch.shape)\n",
    "            print(state_batch.shape)\n",
    "\n",
    "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
    "            next_state_values = torch.zeros(self.batch_size)\n",
    "            \n",
    "            print(next_state_values.shape)\n",
    "            with torch.no_grad():\n",
    "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
    "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
    "                print(non_final_next_states)\n",
    "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
    "                \n",
    "            # Update the model\n",
    "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "            # Update the target Q-network in each 50 steps\n",
    "            self.c += 1\n",
    "            if self.c % 50 == 0:\n",
    "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
    "\n",
    "    def test(self, model_file=None):\n",
    "        # Evaluates the performance of the agent over 20 episodes.\n",
    "\n",
    "        max_t = 1000\n",
    "        state = self.env.reset()\n",
    "        rewards = []\n",
    "\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "            action = self.greedy_policy(q_values)\n",
    "            obs, reward, done, info = self.env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1])\n",
      "torch.Size([32, 10, 9])\n",
      "torch.Size([32])\n",
      "tensor([[[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]],\n",
      "\n",
      "        [[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]],\n",
      "\n",
      "        [[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]],\n",
      "\n",
      "        [[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]],\n",
      "\n",
      "        [[ -9.,  -7.,  -5.,  ...,  -4.,  -6.,  -8.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0., -11.,   0.,  ...,   0., -10.,   0.],\n",
      "         ...,\n",
      "         [  0.,  10.,   0.,  ...,   0.,  11.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  8.,   6.,   4.,  ...,   5.,   7.,   9.]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [32, 2] cannot be broadcast to indexing result of shape [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes_train):\n\u001b[0;32m---> 25\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Evaluate the agent every 10 episodes during training\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[166], line 115\u001b[0m, in \u001b[0;36mDQN_Agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# no next_state_value update if an episode is terminated (next_satate = None)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(non_final_next_states)\n\u001b[0;32m--> 115\u001b[0m     next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mnet(non_final_next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# extract max value\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Update the model\u001b[39;00m\n\u001b[1;32m    118\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma) \u001b[38;5;241m+\u001b[39m reward_batch\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [32, 2] cannot be broadcast to indexing result of shape [32]"
     ]
    }
   ],
   "source": [
    "num_episodes_train = 200\n",
    "num_episodes_test = 20\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(env_name)\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = 4\n",
    "\n",
    "# Plot average performance of 5 trials\n",
    "num_seeds = 5\n",
    "l = num_episodes_train // 10\n",
    "res = np.zeros((num_seeds, l))\n",
    "gamma = 0.99\n",
    "\n",
    "# Loop over multiple seeds\n",
    "for i in tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Create an instance of the DQN_Agent class\n",
    "    agent = DQN_Agent(env_name, lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for m in range(num_episodes_train):\n",
    "        agent.train()\n",
    "\n",
    "        # Evaluate the agent every 10 episodes during training\n",
    "        if m % 10 == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "\n",
    "            # Evaluate the agent's performance over 20 test episodes\n",
    "            G = np.zeros(num_episodes_test)\n",
    "            for k in range(num_episodes_test):\n",
    "                g = agent.test()\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(f\"The test reward for episode {m} is {reward_mean} with a standard deviation of {reward_sd}.\")\n",
    "            reward_means.append(reward_mean)\n",
    "\n",
    "    res[i] = np.array(reward_means)\n",
    "\n",
    "# Plotting the average performance\n",
    "ks = np.arange(l) * 10\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=15)\n",
    "plt.ylabel('Avg. Return', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
